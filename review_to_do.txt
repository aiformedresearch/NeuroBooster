Reasoning:

perchè anche MAE? è più adatto a small dataset, per via del transformer DeiT

se funziona meglio di resnet bene perchè posso implementare i vari paradigmi col transformer. 
Se invece resnet funziona meglio, che è piu probabile perchè abbiamo dataset piccoli, 
allora il problema sarebbe re-implementare masked image modeling con resnet che non è tipico.
Però, se supervised, medbooster e contrastive a parità di architettura funzionano meglio del masked image modeling possono motivare la scelta di non 
implementare il masked image modeling con una arcitettura che usa resnet (che tra l'altro non sarebbe efficiente e nemmeno mai usata).


nel primo esperimento MAE usa un diverso positional embedding, learnable, devo provare con fixed in entrambi i casi

the MAE transformer looks better than SimMIM, the reason is likely to be because DeiT is better suit for small datasets. 

They might tell us that we should train with bigger datasets -> not available.

What if it performs better ResNet? it makes sense because we're talking about smaller datasets. We might do the comparison just for
a single dataset because then we say that we performed the other experiments only with the best architecture. 
 
How to implement the other medical paradigm? we need first to see if it is better resnet or deit. 

What if supervised and medbooster are worse than MAE? compare also medbooster + bbworld

it might need longer training 

potremmo applicare le reti preallenate per un problema di object detection/segmentation


check if longer finetuning makes simim better for age prediction

check if MAE is better with same times



1) add times computation
2) add memory computation
3) add FLOPs computation

make transformer vs resnet instead of Simim

ViT:
- consider testing it only with 10% and 1% labels to be faster? not really, the pretraining needs regardless all the images so the fine tuning is quite fast (?)
- make sure the beit encoder is not masking anything for VICReg and SL
- are the data augmentations happening correctly anyway?
- if explanation that SimMIM did not workproperly was because of the ViT but it works for vicreg we must change discussion


WHEN CHOOSING ANOTHER PARADIGM I MUST TAKE INTO ACCOUNT THAT I SHOULD RE-IMPLEMENT IT WITH THE SAME VISION TRANSFORMER!!! BETTER CHOOSE A TECHNIQUE WHICH EXPLOITS META-data
TO PERFORM CONTRASTIVE SO THAT IT IS SIMILAR TO VICREG AND i CAN USE THE SAME STRATEGY.

ALTERNATIVE: MAE HAD A BETTER PERFORMANCE, BUT IT MIGHT BE EVEN BETTER THAN MEDBOOSTER? ANYWAY, WE COULD USE THAT INSTEAD OF SimMIM

IF WE WANT ALSO TO ADD ANOTHER STRATEGY WE CAN USE ALSO BBWORLD OLD RESULTS? I CAN SAY IT'S THE COMBINATION OF MED-BOOSTER + CONTRASTIVE

CHECK THE LAST LAYERS OF BOTH VERSION OF SUPERVISED RESNET VS TRANSFORMER
3D:
- use 3 axis 2D images  (not so easy to combine their embeddings anyway)
- reduce resolution
- 3D data augmentations
- check if we should expect more or less data needed to train in 3D
- I think it would be enough to apply 3D only comparing in one task Supervised vs MedBooster
- ho solo aggiunto lo script per 3D vision transfrormer ma va integrato bene nel pretraining etc
- se fa cagare il problema potrebbe essere il ViT di SimMIM e non la loss
- usare MAE e il suo ViT? forse fa un masking che rompe piu le scatole?
- we need more data? in realtà meglio comparare a parità di dati perchè è utile avere risposta, inoltre c'è questo esempoio che al contrario per gli stessi risultati servono meno dati: https://pubmed.ncbi.nlm.nih.gov/36829675/
è di solito vero ma nel caso di brain mri potrebbe non esserlo per vari motivi, in particolare nel nostro caso in cui a maggior ragione le features sono piu vere quando il datop è 3D 

additioanl ssl paradigms:
- use best of both worlds or justify one of the algorithms they as it

- replace simmim with MAE and its transformer

The pre-training of a single supervised: 14481MiB /  46068MiB ce ne stanno 3:  43439MiB /  46068MiB

the pre-training of mae: 10444MiB 

finetuning seems 2900MiB

one experiment folder = 342 MB

The pre-training of a single vicreg: 

LA GESTIONE DEL GRADIENTE IN MAE SEMBRA ESSERE LA CHIAVE DEI MIGLIORI RISULTATI!! 21 MAGGIO, COMPARA FINE_TUNING AND FINTE_TUNING_MAE, QUESTO PORTA A DIFFERENZE SOSTANZIALI ALMENO PER UN SEED.

abltation studies ideas:
- compare resnet vs beit vs deit only with supervised training and only one dataset
- re-train SimMIM with gradient management as MAE
- add bbworld as medbooster + contrastive


rimuovi .float 32, traina con stesso management di simim? almeno pretraining?



see if this brings an improvement to CLAM:
from tiatoolbox.models.engine.nucleus_instance_seg import NucleusInstanceSegmentor
import os

model = NucleusInstanceSegmentor(pretrained_model='hovernet_fast-pannuke')

results = model.predict_batch(
    input_dir="/path/to/patches/",
    save_dir="/path/to/output/",
    mode="patch",
    on_gpu=True
)