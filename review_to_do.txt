ablation study: 
main possible reason:
looking at the change that VICReg had by changing the optimizer and lr to LARS, it worked much better. However It also needed more 
epochs for the pretraining. I do the same with the supervised. 
I am launching two ablation studies: one with LARS and one with SIMIM optimization, same amount of pretraining epochs and finetuning epochs so that we can also better compare the convergence times and isolate the causes

if it is not working maybe it is good to use the same simim optim for everyone but increasing the number of epochs instead of using LARS? but in that case vicreg is shitty

It might need even more patience due to the higher instability of the transformer?

the check about when to perform pretraining or not is not working as expected


- optimizer
- longer finetuning
- is the backbone frozen? why does it take the same amount of time??


Reasoning:

perchè anche MAE? è più adatto a small dataset, per via del transformer DeiT

se funziona meglio di resnet bene perchè posso implementare i vari paradigmi col transformer. 
Se invece resnet funziona meglio, che è piu probabile perchè abbiamo dataset piccoli, 
allora il problema sarebbe re-implementare masked image modeling con resnet che non è tipico.
Però, se supervised, medbooster e contrastive a parità di architettura funzionano meglio del masked image modeling possono motivare la scelta di non 
implementare il masked image modeling con una arcitettura che usa resnet (che tra l'altro non sarebbe efficiente e nemmeno mai usata).


nel primo esperimento MAE usa un diverso positional embedding, learnable, devo provare con fixed in entrambi i casi

the MAE transformer looks better than SimMIM, the reason is likely to be because DeiT is better suit for small datasets. 

They might tell us that we should train with bigger datasets -> not available.

What if it performs better ResNet? it makes sense because we're talking about smaller datasets. We might do the comparison just for
a single dataset because then we say that we performed the other experiments only with the best architecture. 
 
How to implement the other medical paradigm? we need first to see if it is better resnet or deit. 

What if supervised and medbooster are worse than MAE? compare also medbooster + bbworld

it might need longer training 

potremmo applicare le reti preallenate per un problema di object detection/segmentation


check if longer finetuning makes simim better for age prediction

check if MAE is better with same times


1) add times computation
2) add memory computation
3) add FLOPs computation

make transformer vs resnet instead of Simim

ViT:
- consider testing it only with 10% and 1% labels to be faster? not really, the pretraining needs regardless all the images so the fine tuning is quite fast (?)
- make sure the beit encoder is not masking anything for VICReg and SL
- are the data augmentations happening correctly anyway?
- if explanation that SimMIM did not workproperly was because of the ViT but it works for vicreg we must change discussion


WHEN CHOOSING ANOTHER PARADIGM I MUST TAKE INTO ACCOUNT THAT I SHOULD RE-IMPLEMENT IT WITH THE SAME VISION TRANSFORMER!!! BETTER CHOOSE A TECHNIQUE WHICH EXPLOITS META-data
TO PERFORM CONTRASTIVE SO THAT IT IS SIMILAR TO VICREG AND i CAN USE THE SAME STRATEGY.

ALTERNATIVE: MAE HAD A BETTER PERFORMANCE, BUT IT MIGHT BE EVEN BETTER THAN MEDBOOSTER? ANYWAY, WE COULD USE THAT INSTEAD OF SimMIM

IF WE WANT ALSO TO ADD ANOTHER STRATEGY WE CAN USE ALSO BBWORLD OLD RESULTS? I CAN SAY IT'S THE COMBINATION OF MED-BOOSTER + CONTRASTIVE

CHECK THE LAST LAYERS OF BOTH VERSION OF SUPERVISED RESNET VS TRANSFORMER
3D:
- use 3 axis 2D images  (not so easy to combine their embeddings anyway)
- reduce resolution
- 3D data augmentations
- check if we should expect more or less data needed to train in 3D
- I think it would be enough to apply 3D only comparing in one task Supervised vs MedBooster
- ho solo aggiunto lo script per 3D vision transfrormer ma va integrato bene nel pretraining etc
- se fa cagare il problema potrebbe essere il ViT di SimMIM e non la loss
- usare MAE e il suo ViT? forse fa un masking che rompe piu le scatole?
- we need more data? in realtà meglio comparare a parità di dati perchè è utile avere risposta, inoltre c'è questo esempoio che al contrario per gli stessi risultati servono meno dati: https://pubmed.ncbi.nlm.nih.gov/36829675/
è di solito vero ma nel caso di brain mri potrebbe non esserlo per vari motivi, in particolare nel nostro caso in cui a maggior ragione le features sono piu vere quando il datop è 3D 

additioanl ssl paradigms:
- use best of both worlds or justify one of the algorithms they as it

- replace simmim with MAE and its transformer

The pre-training of a single supervised: 14481MiB /  46068MiB ce ne stanno 3:  43439MiB /  46068MiB

the pre-training of mae: 10444MiB 

finetuning seems 2900MiB

one experiment folder = 342 MB

The pre-training of a single vicreg: 

LA GESTIONE DEL GRADIENTE IN MAE SEMBRA ESSERE LA CHIAVE DEI MIGLIORI RISULTATI!! 21 MAGGIO, COMPARA FINE_TUNING AND FINTE_TUNING_MAE, QUESTO PORTA A DIFFERENZE SOSTANZIALI ALMENO PER UN SEED.

abltation studies ideas:
- compare resnet vs beit vs deit only with supervised training and only one dataset
- re-train SimMIM with gradient management as MAE
- add bbworld as medbooster + contrastive


rimuovi .float 32, traina con stesso management di simim? almeno pretraining?



see if this brings an improvement to CLAM:
from tiatoolbox.models.engine.nucleus_instance_seg import NucleusInstanceSegmentor
import os

model = NucleusInstanceSegmentor(pretrained_model='hovernet_fast-pannuke')

results = model.predict_batch(
    input_dir="/path/to/patches/",
    save_dir="/path/to/output/",
    mode="patch",
    on_gpu=True
)


to check all the bash scripts:

pgrep -af bash | while read pid cmd; do
    pgrep -P $pid python > /dev/null && echo "Bash PID $pid runs Python: $cmd"
done

to kill the python children based on the word 'vicreg':

ps -ef | grep vicreg | grep bash | awk '{print $2}' | while read pid; do
  pgid=$(ps -o pgid= -p "$pid" | tr -d ' ')
  echo "Killing process group $pgid"
  kill -- -$pgid
done


LARS:

vicreg pretraining: 26591MiB (gpu 0) 

supervised pretraining: 14209MiB (gpu 1) 

mae pretraining: 10265MiB (gpu 2)  

simim pretraining:  15083MiB  (gpu 3) 

medbooster 14200MiB (gpu 0) 

simMIM optim:




space needed in worst case:
5.6G    /Ironman/scratch/Andrea/med-booster/EXPERIMENTS_MRI_augm_21_11/EXPS/seed0  *30 seeds -> 160 GB

I FORGOT ABOUT IT: WE PERFORMED THE PRE-TRAINING ONLY WITH AGE PREDICTION, BECAUSE WE HVE THE FEATURES ONLY FOR THE AGE PREDICTION DATASET!

vicreg	GPU 0, GPU 1
supervised	GPU 1, GPU 3
mae	GPU 0, GPU 3
simim	GPU 2 × 2
medbooster	GPU 3, GPU 2

Fri May 23 16:12:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:3F:00.0 Off |                    0 |
| N/A   36C    P0             80W /  350W |   36853MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    Off |   00000000:56:00.0 Off |                    0 |
| N/A   37C    P0             80W /  350W |   40791MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA L40S                    Off |   00000000:C3:00.0 Off |                    0 |
| N/A   38C    P0             85W /  350W |   44547MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA L40S                    Off |   00000000:DA:00.0 Off |                    0 |
| N/A   36C    P0             83W /  350W |   38711MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     28014      C   python                                      26578MiB |
|    0   N/A  N/A   2970648      C   python                                      10256MiB |
|    1   N/A  N/A   3081214      C   python                                      26572MiB |
|    1   N/A  N/A   3891071      C   python                                      14200MiB |
|    2   N/A  N/A   1484192      C   python                                      15248MiB |
|    2   N/A  N/A   3333184      C   python                                      14200MiB |
|    2   N/A  N/A   3587264      C   python                                      15076MiB |
|    3   N/A  N/A    371601      C   python                                      10274MiB |
|    3   N/A  N/A    946466      C   python                                      14208MiB |
|    3   N/A  N/A   1991785      C   python                                      14206MiB |
+-----------------------------------------------------------------------------------------+



26 05 2025 sembra ci sia un problema con il fine-tuning, come se venisse caricato un modello random e non quello pre-trainato
per vicreg supervised medbooster and simim!!!! infatti la finetuning loss è sempre identica!!!

potrebbe anche essere il motivo per cui simim non da li stessi risultati di prima oppure il motivo per cui prima i risultati
erano cosi brutti, prova a inizializzare random senza caricamento e vedi differenze

i risultati sono diversi anche perchè ho cambiato qualcosina nell'ottimizzazione di simim?


sembra tornare tutto: con pretraining LARS simim fa cagare -> giustifica allenamento con la sua ottimizzazione
vediamo come va con mae, ma a prescindere i risultati di medbooster e supervised sembrano MIGLIORI
possiamo fare un finetuning molto piu veloce
anche mae sembra migliore con la ottimizzazione di simim

ablation study:
train all paradigms with deit ViT
MAE is needed to show that is it not the ViT or the optimization the problem, but it is the task, that maybe needs more data 
compare using the exact same optimization of before for resnet so that we compare the results in terms of only different backbone
fine-tuning as it was

